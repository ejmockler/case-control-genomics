{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFYp82_avzf"
      },
      "source": [
        "# Whole-genome analysis workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPZ8uR95ao7W",
        "outputId": "a1c56261-0150-4f59-e0fa-5e3deea610b5"
      },
      "outputs": [],
      "source": [
        "# ~2 minutes to install \n",
        "#%pip install -U --no-cache-dir scikit-learn scikit-optimize prefect prefect-ray ray plotly openpyxl shap lion_pytorch pytorch_tabnet xgboost neptune pyspark pyarrow dill fastnumbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "T-XkqgwZaBX0",
        "outputId": "1f74b602-b8f5-41c4-d1b4-c1c7ced32b36"
      },
      "outputs": [],
      "source": [
        "from prefect import task, flow\n",
        "from prefect.task_runners import ConcurrentTaskRunner\n",
        "from prefect_ray.task_runners import RayTaskRunner\n",
        "from DillSerializer import DillSerializer\n",
        "import ray\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "\n",
        "!export PREFECT_LOGGING_LEVEL=\"WARNING\"\n",
        "ray.shutdown()\n",
        "parallelRunner = ray.init(\n",
        "  configure_logging=True,\n",
        "  logging_level=logging.ERROR,\n",
        ")\n",
        "parallelRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S04Kzco5ci8n"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    RandomForestClassifier,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from xgboost import XGBClassifier\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "from skopt.space import Categorical, Integer, Real\n",
        "\n",
        "from env import neptune_api_token\n",
        "\n",
        "RadialBasisSVC = SVC\n",
        "RadialBasisSVC.__name__ = \"RadialBasisSVC\"\n",
        "\n",
        "clearHistory = True\n",
        "config = {\n",
        "  'vcfLike': {  \n",
        "    'path': 'Variant_report_NUPs_fixed_2022-03-28.xlsx',             # variant call table with annotations\n",
        "    'sheet': \"all cases vs all controls\",                             # sheet name if Excel spreadsheet\n",
        "    'indexColumn': ['chrom', 'position', 'Gene'],       # header that indexes variants (set as list with multiple columns)\n",
        "    'binarize': True,                           # binarize variants to 0/1, or sum to weigh allele frequency\n",
        "    'minAlleleFrequency': 0.05,           # filter out variants with allele frequency less than this\n",
        "  # 'alleleModel': ['dominant', 'recessive', 'overDominant'],  # biallelic allele models to test on gene sets\n",
        "    'filters': {\n",
        "      } \n",
        "  }, # TODO handle genotypes from related individuals\n",
        "  \n",
        "  'geneSets' : { # TODO gene sets\n",
        "      },\n",
        "  \n",
        "  'tracking': {\n",
        "    'name': 'Nucleoporin genes', # name of the experiment\n",
        "    'entity': 'ejmockler',\n",
        "    'project': 'ALS-NUPs',\n",
        "    'token': neptune_api_token\n",
        "  },\n",
        "\n",
        "  'clinicalTable': {\n",
        "      'path': 'ACWM.xlsx',                      # clinical data as Excel spreadsheet\n",
        "      'idColumn': 'ExternalSampleId',           # genotype ID header\n",
        "      'uniqueIdColumn': 'ExternalSubjectId',    # unique ID for each patient\n",
        "      'labelColumn': 'Subject Group',                # header that has case/control labels\n",
        "      'controlLabels': ['Non-Neurological Control'], # these labels include external sample IDs (like 1000 Genomes)\n",
        "      'caseLabels': ['ALS Spectrum MND'],\n",
        "      'controlAlias': 'control',\n",
        "      'caseAlias': 'case',\n",
        "      'filters': 'pct_european>=0.85',             # filter out nonhomogenous samples with less than 85% European ancestry\n",
        "  },\n",
        "\n",
        "  'externalTable': {\n",
        "      'path': 'igsr-1000 genomes phase 3 release.tsv',  # external sample table\n",
        "      'label': 'control', # case | control | mixed (mixed labels are held out as an external test set)\n",
        "      'idColumn': 'Sample name',                        # sample ID header\n",
        "      'filters': \"`Superpopulation code`=='EUR' & `Population name`!='Finnish'\", # remove finnish samples due to unusual homogeneity (verify w/ PCA)\n",
        "  },\n",
        "\n",
        "  'sampling': {\n",
        "    'bootstrapIterations': 60, \n",
        "    'crossValIterations': 10,   # number of validations per bootstrap iteration\n",
        "    'holdoutSplit': 0.1\n",
        "  },\n",
        "\n",
        "  'modelStack': {\n",
        "    LinearSVC(): {\n",
        "          \"tol\": Real(1e-6, 1e+1, prior=\"log-uniform\"),\n",
        "          \"C\": Real(1e-4, 1e+1, prior=\"log-uniform\"),\n",
        "      },\n",
        "    RadialBasisSVC(probability=True, kernel=\"rbf\"): {\n",
        "        \"tol\": Real(1e-4, 1e+1, prior=\"log-uniform\"),\n",
        "        \"C\": Real(1e-4, 1e+1, prior=\"log-uniform\"),\n",
        "        \"gamma\": Categorical([\"scale\", \"auto\"]),\n",
        "    },\n",
        "    LogisticRegression(penalty=\"l2\", solver=\"saga\"): {\n",
        "          \"tol\": Real(1e-6, 1e+1, prior=\"log-uniform\"),\n",
        "          \"C\": Real(1e-4, 1e+1, prior=\"log-uniform\"),\n",
        "      },\n",
        "      # TabNetClassifier: {\n",
        "      #     \"n_d\": Integer(8, 64),\n",
        "      #     \"n_a\": Integer(8, 64),\n",
        "      #     \"n_steps\": Integer(3, 10),\n",
        "      #     \"lambda_sparse\": Real(1e-4, 1e+1, prior=\"log-uniform\"),\n",
        "      # },\n",
        "      MultinomialNB(): {\"alpha\": Real(1e-10, 1e+1, prior=\"log-uniform\")},\n",
        "      AdaBoostClassifier(): {\n",
        "          \"n_estimators\": Integer(25, 75),\n",
        "          \"learning_rate\": Real(1e-6, 1e+1, prior=\"log-uniform\"),\n",
        "      },\n",
        "      XGBClassifier(): {\n",
        "          \"learning_rate\": Real(1e-6, 1e+1, prior=\"log-uniform\"),\n",
        "          \"n_estimators\": Integer(10, 100),\n",
        "      },\n",
        "      RandomForestClassifier(): { \n",
        "          \"n_estimators\": Integer(75, 200),\n",
        "      },\n",
        "  }\n",
        "}\n",
        " \n",
        "async def remove_all_flows():\n",
        "  from prefect.client import get_client\n",
        "  orion_client = get_client()\n",
        "  flows = await orion_client.read_flows()\n",
        "  for flow in flows:\n",
        "    flow_id = flow.id\n",
        "    print(f\"Deleting flow: {flow.name}, {flow_id}\")\n",
        "    await orion_client._client.delete(f\"/flows/{flow_id}\")\n",
        "    print(f\"Flow with UUID {flow_id} deleted\")\n",
        "\n",
        "if clearHistory: await remove_all_flows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CKlnYjPImk7I",
        "outputId": "13502d78-bfc1-4323-81f1-87f8168ad548"
      },
      "outputs": [],
      "source": [
        "from prefect import unmapped\n",
        "from tqdm import tqdm\n",
        "\n",
        "@task()\n",
        "def filterTable(table, filterString):\n",
        "    if not filterString: return table\n",
        "    print(f\"Filtering: {filterString}\")\n",
        "    filteredTable = table.query(filterString)\n",
        "    return filteredTable\n",
        "\n",
        "@task()\n",
        "def applyAlleleModel(values, columns, genotypeIDs):\n",
        "    # some genotype IDs are subset of column names (or vice versa)\n",
        "    genotypeDict = dict()\n",
        "    resolvedGenotypeIDs = set()\n",
        "    for id in tqdm(genotypeIDs, unit='id'):\n",
        "        for j, column in enumerate(columns):\n",
        "            if id in column or column in id:\n",
        "                # implement allele model\n",
        "                genotypeDict[f\"{column}\"] = [(\n",
        "                                np.sum([int(allele) for allele in genotype.replace(\"'\", \"\").split(\"/\")]) # split by allele delimiter \n",
        "                                if not config[\"vcfLike\"][\"binarize\"] \n",
        "                                else np.clip(\n",
        "                                        np.sum([int(allele) for allele in genotype.replace(\"'\", \"\").split(\"/\")]), \n",
        "                                        a_max=1, a_min=None)\n",
        "                                )\n",
        "                            if any(char.isdigit() for char in genotype)\n",
        "                            else np.nan\n",
        "                            for genotype in values[:,j]\n",
        "                        ]\n",
        "                columns = np.delete(columns, j)\n",
        "                values = np.delete(values, j, axis=1)\n",
        "                resolvedGenotypeIDs.update({id})\n",
        "                break\n",
        "    missingGenotypeIDs = set(genotypeIDs) - resolvedGenotypeIDs  # leftover columns are missing\n",
        "    return genotypeDict, missingGenotypeIDs, resolvedGenotypeIDs\n",
        "\n",
        "@task()\n",
        "def load():\n",
        "    clinicalData = pd.read_excel(config['clinicalTable']['path'], index_col=config['clinicalTable']['idColumn']\n",
        "                                ).drop_duplicates(subset=config['clinicalTable']['uniqueIdColumn'])\n",
        "    externalSamples = pd.read_csv(config['externalTable']['path'], sep='\\t', index_col=config['externalTable']['idColumn'])\n",
        "    annotatedVCF = pd.read_csv(\n",
        "        config['vcfLike']['path'], sep='\\t', dtype=str, index_col=config['vcfLike']['indexColumn'], \n",
        "        ) if \"xlsx\" not in config['vcfLike']['path'] else pd.read_excel(\n",
        "            config['vcfLike']['path'], sheet_name=(config['vcfLike']['sheet'] if config['vcfLike']['sheet'] else None), \n",
        "            dtype=str, na_values=['.'], keep_default_na=False\n",
        "        )\n",
        "    # remove null chromosome positions\n",
        "    annotatedVCF[config['vcfLike']['indexColumn']] = annotatedVCF[config['vcfLike']['indexColumn']].astype(str).replace('', np.nan)\n",
        "    return clinicalData, externalSamples, annotatedVCF.dropna(subset=config['vcfLike']['indexColumn']).set_index(config['vcfLike']['indexColumn'])\n",
        "\n",
        "@flow(task_runner=ConcurrentTaskRunner(), log_prints=True)\n",
        "async def processInputFiles():\n",
        "    clinicalData, externalSamples, annotatedVCF = load()\n",
        "    \n",
        "    filteredClinicalData = filterTable(clinicalData, config['clinicalTable']['filters'])\n",
        "    print(f\"filtered {len(clinicalData) - len(filteredClinicalData)} samples from clinical data\")\n",
        "    filteredExternalSamples = filterTable(externalSamples, config['externalTable']['filters'])\n",
        "    print(f\"filtered {len(externalSamples) - len(filteredExternalSamples)} samples from external data\")\n",
        "    filteredVCF = filterTable(annotatedVCF, config['vcfLike']['filters'])\n",
        "    print(f\"filtered {annotatedVCF.shape[0] - filteredVCF.shape[0]} variants from VCF\")\n",
        "    \n",
        "    caseIDsMask, controlIDsMask = [\n",
        "        filteredClinicalData[config['clinicalTable']['labelColumn']].isin(labels).dropna()\n",
        "        for labels in (config['clinicalTable']['caseLabels'], config['clinicalTable']['controlLabels'])]\n",
        "    \n",
        "    caseIDs = caseIDsMask[caseIDsMask].index.to_numpy()\n",
        "    controlIDs = controlIDsMask[controlIDsMask].index.to_numpy()\n",
        "    if config['externalTable']['label'] == 'case':\n",
        "        caseIDs = np.append(caseIDs, filteredExternalSamples.index.to_numpy())\n",
        "    elif config['externalTable']['label'] == 'control':\n",
        "        controlIDs = np.append(controlIDs, filteredExternalSamples.index.to_numpy())\n",
        "\n",
        "    # cast genotypes as numeric, drop chromosome positions with missing values\n",
        "    caseGenotypeFutures, controlGenotypeFutures = applyAlleleModel.map(\n",
        "            unmapped(filteredVCF.to_numpy()),\n",
        "            unmapped(filteredVCF.columns.to_numpy()), \n",
        "            genotypeIDs=[IDs for IDs in (caseIDs, controlIDs)]\n",
        "        ) \n",
        "    caseGenotypeDict, missingCaseIDs, resolvedCaseIDs = caseGenotypeFutures.result()\n",
        "    controlGenotypeDict, missingControlIDs, resolvedControlIDs = controlGenotypeFutures.result()\n",
        "    \n",
        "    if len(missingCaseIDs) > 0 or len(missingControlIDs) > 0:\n",
        "        for alias, IDs in {\"caseAlias\": missingCaseIDs, \"controlAlias\": missingControlIDs}.items():\n",
        "            print(f\"\\nmissing {len(IDs)} {config['clinicalTable'][alias]} IDs:\\n {IDs}\")\n",
        "            \n",
        "    caseGenotypes = pd.DataFrame.from_dict(caseGenotypeDict)\n",
        "    caseGenotypes.index.name = filteredVCF.index.name\n",
        "    caseGenotypes.index = filteredVCF.index\n",
        "    controlGenotypes = pd.DataFrame.from_dict(controlGenotypeDict)\n",
        "    controlGenotypes.index.name = filteredVCF.index.name\n",
        "    controlGenotypes.index = filteredVCF.index\n",
        "    \n",
        "    caseIDs = resolvedCaseIDs\n",
        "    controlIDs = resolvedControlIDs\n",
        "    \n",
        "    print(f\"\\n{len(caseIDs)} cases:\\n {caseIDs}\")\n",
        "    print(f\"\\n{len(controlIDs)} controls:\\n {controlIDs}\")\n",
        "    # filter allele frequencies\n",
        "    allGenotypes = pd.concat([caseGenotypes.dropna(how='any', axis=0), controlGenotypes.dropna(how='any', axis=0)], axis=1)\n",
        "    filteredGenotypes = allGenotypes.loc[\n",
        "        allGenotypes.gt(0).sum(axis=1).divide(len(allGenotypes.columns)) >= config['vcfLike']['minAlleleFrequency']]\n",
        "    print(f\"Filtered {len(filteredVCF) - len(filteredGenotypes)} alleles with frequency below {'{:.3%}'.format(config['vcfLike']['minAlleleFrequency'])}\")\n",
        "    print(f\"Kept {len(filteredGenotypes)} alleles\")\n",
        "    \n",
        "    caseGenotypes = filteredGenotypes.loc[:,caseGenotypes.columns]\n",
        "    controlGenotypes = filteredGenotypes.loc[:,controlGenotypes.columns]\n",
        "    \n",
        "    return [caseGenotypes, caseIDs, controlGenotypes, controlIDs, filteredClinicalData]\n",
        "\n",
        "caseGenotypes, caseIDs, controlGenotypes, controlIDs, clinicalData = await processInputFiles()\n",
        "print(f\"\\nclinical data:\\n{clinicalData.head()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5xWQrQYTdkSs"
      },
      "source": [
        "## Evaluate model stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "KOm3iWTTdmuj",
        "outputId": "7a7f6f03-4a6d-4cbe-df55-97ab9225f8a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.metrics import RocCurveDisplay, roc_auc_score, auc\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skopt.plots import plot_convergence\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "from fastnumbers import check_real\n",
        "\n",
        "from types import SimpleNamespace\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "from prefect.utilities.annotations import unmapped\n",
        "\n",
        "import neptune\n",
        "from neptune.types import File\n",
        "\n",
        "import shap\n",
        "\n",
        "from inspect import isclass\n",
        "from io import StringIO\n",
        "import traceback\n",
        "\n",
        "# stop API errors when awaiting results\n",
        "# !prefect config set PREFECT_RESULTS_PERSIST_BY_DEFAULT=True\n",
        "\n",
        "def getFeatureImportances(model, data, featureLabels):\n",
        "  \"\"\"Get feature importances from fitted model and create SHAP explainer\"\"\"\n",
        "  modelValuesDF = None\n",
        "  if model.__class__.__name__ == \"MultinomialNB\":\n",
        "      modelValuesDF = pd.DataFrame()\n",
        "      for i, c in enumerate(model.feature_count_[0] if len(model.feature_count_.shape) > 1 else model.feature_count_):\n",
        "          modelValuesDF.loc[i, f\"log_prob_{config['clinicalTable']['controlAlias']}\"] = model.feature_log_prob_[0][i]\n",
        "          modelValuesDF.loc[i, f\"log_prob_{config['clinicalTable']['caseAlias']}\"] = model.feature_log_prob_[1][i]\n",
        "  elif hasattr(model, \"coef_\"):\n",
        "      modelValuesDF = pd.DataFrame()\n",
        "      if len(model.coef_.shape) > 1:\n",
        "        try:\n",
        "          modelValuesDF[f\"feature_importances_{config['clinicalTable']['controlAlias']}\"] = model.coef_[0]\n",
        "          modelValuesDF[f\"feature_importances_{config['clinicalTable']['caseAlias']}\"] = model.coef_[1]\n",
        "        except IndexError:\n",
        "          modelValuesDF[f\"feature_importances_{config['clinicalTable']['caseAlias']}\"] = model.coef_[0]                 \n",
        "      else:\n",
        "        modelValuesDF[f\"feature_importances\"] = model.coef_[0]\n",
        "  elif hasattr(model, \"feature_importances_\"):\n",
        "      modelValuesDF = pd.DataFrame()\n",
        "      modelValuesDF[f\"feature_importances_{config['clinicalTable']['caseAlias']}\"] = model.feature_importances_\n",
        "\n",
        "  if type(modelValuesDF) == pd.DataFrame:\n",
        "      modelValuesDF.index = featureLabels\n",
        "      \n",
        "  # Cluster correlated and hierarchical features using masker\n",
        "  masker = shap.maskers.Partition(data, clustering=\"correlation\")\n",
        "  \n",
        "  shapExplainer = shap.explainers.Permutation(\n",
        "    model.predict_proba if hasattr(model, \"predict_proba\") \n",
        "    else model.predict, \n",
        "    masker, \n",
        "    feature_names=[\"_\".join(label)\n",
        "      for label in featureLabels])\n",
        "  shapValues = shapExplainer(data)\n",
        "  return modelValuesDF, shapValues, shapExplainer\n",
        "\n",
        "\n",
        "@task()\n",
        "def plotCalibration(title, labelsPredictionsByInstance):\n",
        "  # code from https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html\n",
        "  fig, ax_calibration_curve = plt.subplots(figsize=(10, 10))\n",
        "  colors = plt.cm.get_cmap(\"Dark2\")\n",
        "\n",
        "  calibration_displays = {}\n",
        "  for i, (name, (labels, predictions)) in enumerate(labelsPredictionsByInstance.items()):\n",
        "      display = CalibrationDisplay.from_predictions(\n",
        "          [config[\"clinicalTable\"][\"caseAlias\"] if label == 1 else label \n",
        "            for label in labels],\n",
        "          predictions,\n",
        "          pos_label=config[\"clinicalTable\"][\"caseAlias\"],\n",
        "          n_bins=10,\n",
        "          name=name,\n",
        "          ax=ax_calibration_curve,\n",
        "          color=colors(i),\n",
        "      )\n",
        "      calibration_displays[name] = display\n",
        "\n",
        "  ax_calibration_curve.grid()\n",
        "  ax_calibration_curve.set_title(title)\n",
        "\n",
        "  # Add histogram\n",
        "  # grid_positions = [(i+2,j) for i in range(len(predictionsByModelName.keys())//2) for j in range(2)]\n",
        "  # for i, modelName in enumerate(predictionsByModelName.keys()):\n",
        "  #     row, col = grid_positions[i]\n",
        "  #     ax = fig.add_subplot(gs[row, col])\n",
        "  #     ax.hist(\n",
        "  #         calibration_displays[modelName].y_prob,\n",
        "  #         range=(0, 1),\n",
        "  #         bins=10,\n",
        "  #         label=modelName,\n",
        "  #         color=colors(i),\n",
        "  #     )\n",
        "  #     ax.set(title=modelName, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  return fig\n",
        "\n",
        "@task()\n",
        "def plotAUC(title, labelsPredictionsByInstance):\n",
        "  # trace AUC for each set of predictions\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  for name, (labels, predictions) in labelsPredictionsByInstance.items():\n",
        "    # plot ROC curve for this fold\n",
        "    viz = RocCurveDisplay.from_predictions([\n",
        "                          config[\"clinicalTable\"][\"caseAlias\"] if label == 1 else label \n",
        "                          for label in labels], \n",
        "                        predictions,\n",
        "                        name=name,\n",
        "                        pos_label=config[\"clinicalTable\"][\"caseAlias\"],\n",
        "                        alpha=0.6, lw=2, ax=ax)\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "    \n",
        "  # summarize ROCs per fold and plot standard deviation\n",
        "  ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "      label='Chance', alpha=.8)\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "  ax.plot(mean_fpr, mean_tpr, color='b',\n",
        "          label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "          lw=4, alpha=.8)\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "  ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                  label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "  ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
        "        title=title)\n",
        "  ax.legend(loc=\"lower right\")\n",
        "  ax.set(title=title)\n",
        "  return fig\n",
        "\n",
        "\n",
        "@task()\n",
        "def plotConfusionMatrix():\n",
        "  pass\n",
        "\n",
        "@task()\n",
        "def plotSampleAccuracy():\n",
        "  pass\n",
        "\n",
        "@task()\n",
        "def plotOptimizer(title, resultsByInstance):\n",
        "  # code from https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  gs = GridSpec(2, 2)\n",
        "  colors = plt.cm.get_cmap(\"Dark2\")\n",
        "  ax_convergence = fig.add_subplot(gs[:2, :2])\n",
        "  plot_convergence(*[\n",
        "    (modelName, result) for modelName, result in resultsByInstance.items()], \n",
        "    ax=ax_convergence, color=colors)\n",
        "  ax_convergence.set(title=title)\n",
        "  plt.tight_layout()\n",
        "  return fig\n",
        "\n",
        "@task()\n",
        "def prepareDatasets(caseGenotypes, controlGenotypes, verbose=True):\n",
        "    caseIDs = caseGenotypes.columns\n",
        "    controlIDs = controlGenotypes.columns\n",
        "    # store number of cases & controls\n",
        "    caseControlCounts = [len(caseIDs), len(controlIDs)]\n",
        "    # determine which has more samples\n",
        "    labeledIDs = [caseIDs, controlIDs]\n",
        "    majorIDs = labeledIDs[np.argmax(caseControlCounts)]\n",
        "    minorIDs = labeledIDs[np.argmin(caseControlCounts)]\n",
        "    # downsample larger group to match smaller group\n",
        "    majorIndex = np.random.choice(np.arange(len(majorIDs)), min(caseControlCounts), replace=False)\n",
        "    \n",
        "    excessMajorIDs, balancedMajorIDs = [], []\n",
        "    for index, id in enumerate(majorIDs):\n",
        "      if index in majorIndex:\n",
        "        balancedMajorIDs.append(id)\n",
        "      else:\n",
        "        excessMajorIDs.append(id)\n",
        "    \n",
        "    allGenotypes = pd.concat([caseGenotypes, controlGenotypes], axis=1)\n",
        "    \n",
        "    genotypeExcessIDs, genotypeTrainIDs = [], []\n",
        "    # match IDs between genotype and clinical data; dataframe labels have label suffixes\n",
        "    unmatchedTrainIDs = balancedMajorIDs + minorIDs\n",
        "    for label in tqdm(allGenotypes.columns, desc=\"Matching IDs\", unit=\"ID\"):\n",
        "      for setType in [\"excess\", \"train\"]:\n",
        "        idSet = excessMajorIDs if setType == \"excess\" else unmatchedTrainIDs\n",
        "        for i, id in enumerate(idSet):\n",
        "          if (id in label) or (label in id): \n",
        "            if setType == \"train\":\n",
        "              genotypeTrainIDs.append(label)\n",
        "            elif setType == \"excess\":\n",
        "              genotypeExcessIDs.append(label)\n",
        "            idSet = np.delete(idSet, i)\n",
        "            break\n",
        "          \n",
        "    if verbose:\n",
        "      print(f\"\\n{len(genotypeTrainIDs)} for training:\\n{genotypeTrainIDs}\")\n",
        "      print(f\"\\n{len(genotypeExcessIDs)} are excess:\\n{genotypeExcessIDs}\")\n",
        "      print(f\"\\nVariant count: {len(allGenotypes.index)}\")\n",
        "      \n",
        "    samples = allGenotypes.loc[:, genotypeTrainIDs] \n",
        "    excessMajorSamples = allGenotypes.loc[:, genotypeExcessIDs]\n",
        "\n",
        "    variantIndex = list(allGenotypes.index)\n",
        "    scaler = MinMaxScaler()\n",
        "    embedding = {\n",
        "        'sampleIndex': genotypeTrainIDs,\n",
        "        'labels': np.array([1 if id in caseIDs else 0 for id in genotypeTrainIDs]),\n",
        "        'samples': scaler.fit_transform(samples).transpose(), # samples are now rows (samples, variants)\n",
        "        'excessMajorIndex': genotypeExcessIDs,\n",
        "        'excessMajorLabels': [1 if id in caseIDs else 0 for id in genotypeExcessIDs],\n",
        "        'excessMajorSamples': scaler.fit_transform(excessMajorSamples).transpose(),\n",
        "        'variantIndex': variantIndex,\n",
        "    }\n",
        "    return embedding\n",
        "\n",
        "@task()\n",
        "def evaluate(trainIndices, testIndices, model, labels, samples, variantIndex, sampleIndex, parameterSpace, cvIterator):\n",
        "    fittedOptimizer = optimizeHyperparameters(\n",
        "        samples[trainIndices], labels[trainIndices], model, \n",
        "        parameterSpace, cvIterator, 'neg_mean_squared_error')\n",
        "    model.set_params(**fittedOptimizer.best_params_)\n",
        "    model.fit(samples[trainIndices], labels[trainIndices])\n",
        "    # get model prediction probabilities\n",
        "    try:\n",
        "      probabilities = model.predict_proba(samples[testIndices])\n",
        "    except AttributeError:\n",
        "      probabilities = model.predict(samples[testIndices])\n",
        "      if len(probabilities.shape) <= 1:\n",
        "        probabilities = np.array([[1 - p, p] for p in probabilities])\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "    modelValues, shapValues, shapExplainer = getFeatureImportances(\n",
        "      model, samples[testIndices], variantIndex)\n",
        "    globalExplanations = modelValues\n",
        "    localExplanations = shapValues\n",
        "    trainLabels = np.array(labels[trainIndices])\n",
        "    testLabels = np.array(labels[testIndices])\n",
        "    trainIDs = np.array([sampleIndex[i] for i in trainIndices]) \n",
        "    testIDs = np.array([sampleIndex[i] for i in testIndices])\n",
        "    return globalExplanations, localExplanations, probabilities, predictions, testLabels, trainLabels, trainIDs, testIDs, fittedOptimizer, shapExplainer\n",
        "\n",
        "def optimizeHyperparameters(samples, labels, model, parameterSpace, cvIterator, metricFunction, n_jobs=1):\n",
        "  # hyperparameter search (inner cross-validation)\n",
        "  optimizer = BayesSearchCV(\n",
        "    model, parameterSpace, cv=cvIterator, n_jobs=n_jobs, n_points=4,\n",
        "    return_train_score=True, n_iter=100, scoring=metricFunction)\n",
        "  # train / optimize parameters\n",
        "  optimizer.fit(samples, labels)\n",
        "  return optimizer\n",
        "\n",
        "def serializeDataFrame(dataframe):\n",
        "  stream = StringIO()\n",
        "  dataframe.to_csv(stream)\n",
        "  return File.from_stream(stream, extension='csv')\n",
        "\n",
        "@flow(\n",
        "  task_runner=RayTaskRunner(\n",
        "    init_kwargs={'address': parallelRunner.address_info['address'], 'configure_logging': True, 'logging_level': logging.WARN}\n",
        "  ))\n",
        "async def classify():\n",
        "  # caseGenotypes, caseIDs, controlGenotypes, controlIDs, clinicalData = await processInputFiles()\n",
        "  outerCvIterator = StratifiedKFold(n_splits=config['sampling']['crossValIterations'], shuffle=False)\n",
        "  innerCvIterator = outerCvIterator\n",
        "  results = {}\n",
        "  projectTracker = neptune.init_project(project=f'{config[\"tracking\"][\"entity\"]}/{config[\"tracking\"][\"project\"]}', api_token=config['tracking']['token'])\n",
        "  for i in tqdm(range(config['sampling']['bootstrapIterations']), unit='cohort'):\n",
        "    embedding = prepareDatasets(caseGenotypes, controlGenotypes, verbose=(True if i == 0 else False))\n",
        "    deserializedIDs = list()\n",
        "    for id in embedding['sampleIndex']:\n",
        "      deserializedIDs.extend(id.split(\"__\"))\n",
        "    totalSampleCount = len(embedding['samples'])\n",
        "    caseCount = np.count_nonzero(embedding['labels'])\n",
        "    print(f\"{totalSampleCount} samples\\n\")\n",
        "    print(f\"{caseCount} cases\\n\")\n",
        "    print(f\"{totalSampleCount - caseCount} controls\\n\")\n",
        "    results[i] = {}\n",
        "    results['samples'] = {}\n",
        "    results['labels'] = {}\n",
        "    for j, (model, parameterSpace) in enumerate(config['modelStack'].items()):\n",
        "      if model.__class__.__name__ not in results:\n",
        "        results[i][model.__class__.__name__] = {\"predictions\": [], \"optimizeResults\": []} \n",
        "      current = {}\n",
        "      # check if model is initialized\n",
        "      if isclass(model):\n",
        "        if model.__name__ == 'TabNetClassifier':\n",
        "          model = model(verbose=False, optimizer_fn=Lion)\n",
        "      print(f\"Iteration {i+1} with model {model.__class__.__name__}\")\n",
        "      runTracker = neptune.init_run(project=f'{config[\"tracking\"][\"entity\"]}/{config[\"tracking\"][\"project\"]}', api_token=config['tracking']['token'])\n",
        "      runTracker['sys/tags'].add(model.__class__.__name__)\n",
        "      runTracker['bootstrapIteration'] = i+1\n",
        "      runTracker[\"config\"] = {key: (item if check_real(item) or isinstance(item, str) else str(item)) for key, item in config.items()}\n",
        "      runTracker['embedding'].upload(serializeDataFrame(pd.DataFrame(data=embedding[\"samples\"], columns=embedding[\"variantIndex\"], index=embedding[\"sampleIndex\"])))\n",
        "      runTracker['clinicalData'].upload(serializeDataFrame(clinicalData.loc[clinicalData.index.isin(deserializedIDs)]))\n",
        "      # outer cross-validation\n",
        "      crossValIndices = np.array([\n",
        "          (cvTrainIndices, cvTestIndices) \n",
        "          for (cvTrainIndices, cvTestIndices) in outerCvIterator.split(embedding['samples'], embedding['labels'])])\n",
        "      current[\"trainIndices\"] = crossValIndices[:,0]\n",
        "      current[\"testIndices\"] = crossValIndices[:,1]\n",
        "      # this could be parallelized more efficiently since Shapely explanations are independent\n",
        "      outerCrossValFutures = evaluate.map(\n",
        "          trainIndices=crossValIndices[:,0], \n",
        "          testIndices=crossValIndices[:,1],\n",
        "          model=unmapped(model), \n",
        "          labels=unmapped(embedding['labels']), \n",
        "          samples=unmapped(embedding['samples']), \n",
        "          variantIndex=unmapped(embedding['variantIndex']), \n",
        "          sampleIndex=unmapped(embedding['sampleIndex']), \n",
        "          parameterSpace=unmapped(parameterSpace), \n",
        "          cvIterator=unmapped(innerCvIterator))\n",
        "      outerCrossValResults = zip(*[fold.result() for fold in outerCrossValFutures])\n",
        "      resultNames = [\"globalExplanations\", \"localExplanations\", \"probabilities\", \"predictions\", \"testLabels\", \"trainLabels\", \"trainIDs\", \"testIDs\", \"fittedOptimizers\", \"shapExplainers\"]\n",
        "      current = {**current, **{name: result for name, result in zip(resultNames, outerCrossValResults)}}\n",
        "      \n",
        "      # plot AUC & elbow\n",
        "      runTracker[\"modelParams\"] = {k+1: current[\"fittedOptimizers\"][k].best_params_ for k in range(config['sampling']['crossValIterations'])}\n",
        "      plotSubtitle = f\"\"\"\n",
        "            {config[\"tracking\"][\"name\"]}, {embedding[\"samples\"].shape[1]} variants\n",
        "            Minor allele frequency over {'{:.1%}'.format(config['vcfLike']['minAlleleFrequency'])}\n",
        "            {np.count_nonzero(current['testLabels'][k])} {config[\"clinicalTable\"][\"caseAlias\"]}s, {len(current['testLabels'][k]) - np.count_nonzero(current['testLabels'][k])} {config[\"clinicalTable\"][\"controlAlias\"]}s\"\"\"\n",
        "      runTracker['plots/aucPlot'] = plotAUC(f\"\"\"\n",
        "        Receiver Operating Characteristic (ROC) Curve\n",
        "        {model.__class__.__name__} with {config['sampling']['crossValIterations']}-fold cross-validation\n",
        "        {plotSubtitle}\n",
        "        \"\"\",\n",
        "        {f\"Fold {k+1}\": (current[\"testLabels\"][k], np.array(current[\"probabilities\"][k])[:,1])\n",
        "         if len(current[\"probabilities\"][k][0].shape) >= 1 \n",
        "         else (current[\"testLabels\"][k], current[\"probabilities\"][k]) \n",
        "         for k in range(config['sampling']['crossValIterations'])},\n",
        "        )\n",
        "      runTracker['plots/convergencePlot'] = plotOptimizer(f\"\"\"\n",
        "        Hyperparameter convergence, mean squared error\n",
        "        {model.__class__.__name__} with {config['sampling']['crossValIterations']}-fold cross-validation\n",
        "        {plotSubtitle}\n",
        "        \"\"\",\n",
        "        {f\"Fold {k+1}\": [result for result in current[\"fittedOptimizers\"][k].optimizer_results_]\n",
        "         for k in range(config['sampling']['crossValIterations'])},\n",
        "        )\n",
        "      \n",
        "      # update model metrics\n",
        "      current[\"testAUC\"] = [roc_auc_score(labels, (probabilities[:,1] if len(probabilities.shape) > 1 else probabilities))\n",
        "        for labels, probabilities in zip(current[\"testLabels\"], current[\"probabilities\"])]\n",
        "      runTracker[\"meanAUC\"] = np.mean(current[\"testAUC\"])\n",
        "      # update sample metrics\n",
        "      sampleResults = {}\n",
        "      for fold in range(config['sampling']['crossValIterations']):\n",
        "        for j, sampleID in enumerate(current['testIDs'][fold]):\n",
        "          try:\n",
        "            results['samples'][sampleID] += current[\"probabilities\"][fold][j]\n",
        "          except KeyError:\n",
        "            results['samples'][sampleID] = [current[\"probabilities\"][fold][j]]\n",
        "          finally:\n",
        "            results['labels'][sampleID] = current[\"testLabels\"][fold][j]\n",
        "      results[i][model.__class__.__name__] = current\n",
        "      \n",
        "      runTracker['shapExplanationsPerFold'].upload(File.as_pickle(current[\"localExplanations\"]))\n",
        "      runTracker['shapExplainersPerFold'].upload(File.as_pickle(current[\"shapExplainers\"]))\n",
        "      runTracker['trainIDs'].upload(File.as_pickle(current[\"trainIDs\"]))\n",
        "      runTracker['testIDs'].upload(File.as_pickle(current[\"testIDs\"]))\n",
        "      runTracker['testLabels'].upload(File.as_pickle(current[\"testLabels\"]))\n",
        "      runTracker['trainLabels'].upload(File.as_pickle(current[\"trainLabels\"]))\n",
        "      \n",
        "      # plot feature importance\n",
        "      for j in range(config[\"sampling\"][\"crossValIterations\"]):\n",
        "        if current[\"globalExplanations\"][j] is not None:\n",
        "          runTracker[f\"globalFeatureImportance/{j+1}\"].upload(serializeDataFrame(current[\"globalExplanations\"][j]))\n",
        "        localExplanations = current[\"localExplanations\"][j]\n",
        "        caseExplanations = localExplanations\n",
        "        caseExplanations.values = caseExplanations.values[:,:,1] if len(caseExplanations.values.shape) > 2 else caseExplanations.values\n",
        "        heatmap = plt.figure()\n",
        "        plt.title(f\"\"\"\n",
        "          Shapely explanations from {model.__class__.__name__}\n",
        "          Fold {j+1}\n",
        "          {plotSubtitle}\n",
        "          \"\"\") \n",
        "        shap.plots.heatmap(caseExplanations, show=False)\n",
        "        runTracker[f\"plots/featureHeatmap/{j+1}\"] = heatmap\n",
        "        plt.close(heatmap)\n",
        "        labelsProbabilities = ((current[\"testLabels\"][j], np.array(current[\"probabilities\"][j])[:,1])\n",
        "          if len(current[\"probabilities\"][j][0].shape) >= 1 \n",
        "          else (current[\"testLabels\"][j], current[\"probabilities\"][j]))\n",
        "        stdDeviation = np.std((labelsProbabilities[1] - labelsProbabilities[0])**2)\n",
        "        for k in range(len(current[\"testIDs\"][j])):\n",
        "          probability = labelsProbabilities[1][k] if isinstance(labelsProbabilities[1][k], np.ndarray) else labelsProbabilities[1][k]\n",
        "          label = labelsProbabilities[0][k] if isinstance(labelsProbabilities[0][k], np.ndarray) else labelsProbabilities[0][k]\n",
        "          if np.absolute((probability - label)**2) <= stdDeviation:\n",
        "            sampleID=current['testIDs'][j][k]\n",
        "            waterfallPlot = plt.figure()\n",
        "            plt.title(f\"\"\"\n",
        "              {sampleID}\n",
        "              Shapely explanations from {model.__class__.__name__}\n",
        "              Fold {j+1}\n",
        "              {plotSubtitle}\n",
        "              \"\"\")\n",
        "            # patch parameter bug: https://github.com/slundberg/shap/issues/2362\n",
        "            to_pass = SimpleNamespace(**{\n",
        "                            'values': localExplanations[k].values,\n",
        "                            'data': localExplanations[k].data,\n",
        "                            'display_data': None,\n",
        "                            'feature_names': localExplanations.feature_names,\n",
        "                            'base_values': localExplanations[k].base_values[current['testLabels'][j][k]] if len(localExplanations[k].base_values.shape) == 1 else localExplanations[k].base_values, \n",
        "              })\n",
        "            shap.plots.waterfall(to_pass, show=False)\n",
        "            try:\n",
        "              runTracker[f\"plots/samples/{j+1}/{sampleID}\"] = waterfallPlot\n",
        "            except Exception as e:\n",
        "              runTracker[f\"plots/samples/{j+1}/{sampleID}\"] = f\"\"\"failed to plot: {traceback.format_exc()}\"\"\"\n",
        "            plt.close(waterfallPlot)\n",
        "\n",
        "      runTracker[\"sampleResults\"].upload(serializeDataFrame(pd.DataFrame.from_dict({\n",
        "        \"probability\": [probability[1] for foldResults in current[\"probabilities\"] for probability in foldResults], \n",
        "        \"id\": [id for foldResults in current[\"testIDs\"] for id in foldResults]}, dtype=object).set_index(\"id\")))\n",
        "      plt.close('all')\n",
        "      runTracker.stop()\n",
        "  \n",
        "  labelsProbabilitiesByModelName = dict()\n",
        "  for modelName in results[0].keys():\n",
        "    labelsProbabilitiesByModelName[modelName] = [[], []]\n",
        "    for k in range(config['sampling']['bootstrapIterations']):\n",
        "      # append labels\n",
        "      labelsProbabilitiesByModelName[modelName][0] = np.hstack(\n",
        "        [labelsProbabilitiesByModelName[modelName][0], \n",
        "        np.concatenate(results[k][modelName][\"testLabels\"])])\n",
        "      # append probabilities\n",
        "      labelsProbabilitiesByModelName[modelName][1] = np.hstack(\n",
        "        [labelsProbabilitiesByModelName[modelName][1], \n",
        "        np.concatenate(results[k][modelName][\"probabilities\"])[:,1] \n",
        "          if len(results[k][modelName][\"probabilities\"][0].shape) >= 1 \n",
        "          else np.concatenate(results[k][modelName][\"probabilities\"])])\n",
        "      \n",
        "  seenCaseCount, seenControlCount = 0, 0\n",
        "  sampleResults = []\n",
        "  \n",
        "  for sampleID in results['samples'].keys():\n",
        "    flattenedProbabilities = np.array([\n",
        "      result[1] if len(caseExplanations.values.shape) > 2 \n",
        "      else result \n",
        "      for foldResult in results['samples'][sampleID] \n",
        "      for result in foldResult]) \n",
        "    sampleResults += [{\n",
        "      \"probability\": results['samples'][sampleID][:1],\n",
        "      \"accuracy\": np.mean([np.ceil(caseProbability) == results['labels'][sampleID] \n",
        "                            for caseProbability in flattenedProbabilities]),\n",
        "      \"id\": sampleID,\n",
        "    }]\n",
        "    if results[\"labels\"][sampleID] == 1:\n",
        "      seenCaseCount += 1\n",
        "    else:\n",
        "      seenControlCount += 1\n",
        "      \n",
        "  plotSubtitle = f\"\"\"{config['sampling']['crossValIterations']}x cross-validation over {config['sampling']['bootstrapIterations']} bootstrap iterations\n",
        "  {config[\"tracking\"][\"name\"]}, {embedding[\"samples\"].shape[1]} variants\n",
        "  Minor allele frequency over {'{:.1%}'.format(config['vcfLike']['minAlleleFrequency'])}\n",
        "  {seenCaseCount} cases, {seenControlCount} controls\n",
        "  \"\"\"\n",
        "  # print(f'preds 1: {results[0][\"TabNetClassifier\"][\"predictions\"]}\\n')\n",
        "  # print(\"labels:\")\n",
        "  # projectTracker[\"sampleResults\"].upload(serializeDataFrame(pd.DataFrame.from_records(sampleResults, index=\"id\")))\n",
        "  projectTracker[\"aucPlot\"].upload(plotAUC(f\"\"\"\n",
        "    Receiver Operating Characteristic (ROC) Curve\n",
        "    {plotSubtitle}\n",
        "    \"\"\",\n",
        "    labelsProbabilitiesByModelName,\n",
        "    ))\n",
        "  \n",
        "  projectTracker[\"calibrationPlot\"].upload(File.as_image(plotCalibration(f\"\"\"\n",
        "    Calibration Curve\n",
        "    {plotSubtitle}\n",
        "    \"\"\",\n",
        "    labelsProbabilitiesByModelName,\n",
        "    )))\n",
        "    \n",
        "  projectTracker[\"convergencePlot\"].upload(File.as_image(plotOptimizer(f\"\"\"\n",
        "    Convergence Plot\n",
        "    {plotSubtitle}\n",
        "    \"\"\",\n",
        "    {modelName: [result\n",
        "                for k in range(config['sampling']['bootstrapIterations'])\n",
        "                for foldOptimizer in results[k][modelName][\"fittedOptimizers\"]\n",
        "                for result in foldOptimizer.optimizer_results_\n",
        "                ] \n",
        "      for modelName in results[0].keys() if modelName != \"testLabels\"\n",
        "    })))\n",
        "  projectTracker.stop()\n",
        "  return results\n",
        "\n",
        "results = await classify()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8de2bb8cfc0129dc4ad0d33ebf53f4c81d236710e7f8da07ef8e91d061d131b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
