{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"4g\")  # Set to your desired heap size\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcfLikePath='./als_1kg.exon.num.txt'\n",
    "\n",
    "genotypes = ps.read_csv(vcfLikePath, sep='\\t', index_col=['chrom', 'position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(654394, 5923)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genotypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gene', 'Func.knownGene', 'Func.refGene', 'Func.ensGene', 'ExonicFunc',\n",
       "       'AAChange.knownGene', 'AAChange.refGene', 'AAChange.ensGene', 'FILTER',\n",
       "       'REF',\n",
       "       ...\n",
       "       'CGND-HDA-02442', 'CGND-HDA-02438', 'CGND-HDA-02445', 'CGND-HDA-02693',\n",
       "       'CGND-HDA-02446', 'CGND-HDA-02444', 'CGND-HDA-02439', 'CGND-HDA-02462',\n",
       "       'CGND-HDA-02688', 'CGND-HDA-02440'],\n",
       "      dtype='object', length=5923)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genotypes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gene sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJsonFile(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_list` loads all data into the driver's memory. It should only be used if the resulting list is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "geneSets = {\n",
    "    'alsod': ps.read_csv('./alsodList.csv', sep='\\t')['Gene symbol'].tolist(),\n",
    "    'cardiac': readJsonFile('./BIOCARTA_ALK_PATHWAY.v2023.1.Hs.json')['BIOCARTA_ALK_PATHWAY']['geneSymbols'] \n",
    "        + readJsonFile('./BIOCARTA_ACE2_PATHWAY.v2023.1.Hs.json')['BIOCARTA_ACE2_PATHWAY']['geneSymbols'] \n",
    "        + readJsonFile('./BIOCARTA_AT1R_PATHWAY.v2023.1.Hs.json')['BIOCARTA_AT1R_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_CARDIACEGF_PATHWAY.v2023.1.Hs.json')['BIOCARTA_CARDIACEGF_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_FLUMAZENIL_PATHWAY.v2023.1.Hs.json')['BIOCARTA_FLUMAZENIL_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_GCR_PATHWAY.v2023.1.Hs.json')['BIOCARTA_GCR_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_HDAC_PATHWAY.v2023.1.Hs.json')['BIOCARTA_HDAC_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_NFAT_PATHWAY.v2023.1.Hs.json')['BIOCARTA_NFAT_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_PGC1A_PATHWAY.v2023.1.Hs.json')['BIOCARTA_PGC1A_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_PITX2_PATHWAY.v2023.1.Hs.json')['BIOCARTA_PITX2_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_AMI_PATHWAY.v2023.1.Hs.json')['BIOCARTA_AMI_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_P53HYPOXIA_PATHWAY.v2023.1.Hs.json')['BIOCARTA_P53HYPOXIA_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_NO1_PATHWAY.v2023.1.Hs.json')['BIOCARTA_NO1_PATHWAY']['geneSymbols']\n",
    "        + readJsonFile('./BIOCARTA_HIF_PATHWAY.v2023.1.Hs.json')['BIOCARTA_HIF_PATHWAY']['geneSymbols']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGT', 'ALAD', 'ALS2', 'ALS3', 'ALS7', 'ANG', 'ANXA11', 'APEX1', 'APOE', 'AR', 'ARHGEF28', 'ARPP21', 'ATXN1', 'ATXN2', 'B4GALT6', 'BCL11B', 'BCL6', 'C9orf72', 'CAMTA1', 'CAV1', 'CAV2', 'CCNF', 'CCS', 'CDH13', 'CDH22', 'CFAP410', 'CHCHD10', 'CHGB', 'CHMP2B', 'CNTF', 'CNTN4', 'CNTN6', 'CRIM1', 'CRYM', 'CSNK1G3', 'CST3', 'CX3CR1', 'CYP2D6', 'DAO', 'DCTN1', 'DIAPH3', 'DISC1', 'DNAJC7', 'DNMT3A', 'DNMT3B', 'DOC2B', 'DPP6', 'DYNC1H1', 'EFEMP1', 'ELP3', 'ENAH', 'EphA3', 'EPHA4', 'ERBB4', 'ERLIN1', 'EWSR1', 'FEZF2', 'FGGY', 'FIG4', 'FUS', 'GARS', 'GLE1', 'GLT8D1', 'GPX3', 'GRB14', 'GRN', 'HEXA', 'HFE', 'HNRNPA1', 'HNRNPA2B1', 'ITPR2', 'KDR', 'KIF5A', 'KIFAP3', 'LIF', 'LIPC', 'LMNB1', 'LOX', 'LUM', 'MAOB', 'MAPT', 'MATR3', 'MOBP', 'MTND2P1', 'NAIP', 'NEFH', 'NEFL', 'NEK1', 'NETO1', 'NIPA1', 'NT5C1A', 'ODR4', 'OGG1', 'OMA1', 'OPTN', 'PARK7', 'PCP4', 'PFN1', 'PLEKHG5', 'PNPLA6', 'PON1', 'PON2', 'PON3', 'PRPH', 'PSEN1', 'PVR', 'RAMP3', 'RBMS1', 'RFTN1', 'RNASE2', 'RNF19A', 'SARM1', 'SCFD1', 'SCN7A', 'SELL', 'SEMA6A', 'SETX', 'SIGMAR1', 'SLC1A2', 'SLC39A11', 'SLC52A3', 'SMN1', 'SMN2', 'SNCG', 'SOD1', 'SOD2', 'SOX5', 'SPAST', 'SPG11', 'SPG7', 'SQSTM1', 'SS18L1', 'SUSD1', 'SYNE1', 'SYT9', 'TAF15', 'TARDBP', 'TBK1', 'TIA1', 'TNIP1', 'TRPM7', 'TUBA4A', 'UBQLN1', 'UBQLN2', 'UNC13A', 'VAPB', 'VCP', 'VDR', 'VEGFA', 'VPS54', 'VRK1', 'ZFP64', 'ZNF512B', 'ZNF746']\n",
      "['ACVR1', 'APC', 'ATF2', 'AXIN1', 'BMP10', 'BMP2', 'BMP4', 'BMP5', 'BMP7', 'BMPR1A', 'BMPR2', 'CHRD', 'CTNNB1', 'DVL1', 'FZD1', 'GATA4', 'GSK3B', 'HNF1A', 'MAP3K7', 'MEF2C', 'MYL2', 'NKX2-5', 'NOG', 'NPPA', 'NPPB', 'RFC1', 'SMAD1', 'SMAD4', 'SMAD5', 'SMAD6', 'TGFB1', 'TGFB2', 'TGFB3', 'TGFBR1', 'TGFBR2', 'TGFBR3', 'WNT1', 'ACE', 'ACE2', 'AGT', 'AGTR1', 'AGTR2', 'CMA1', 'COL4A1', 'COL4A2', 'COL4A3', 'COL4A4', 'COL4A5', 'COL4A6', 'REN', 'AGT', 'AGTR1', 'ATF2', 'CALM1', 'CALM2', 'CALM3', 'EGFR', 'ELK1', 'GNAQ', 'GRB2', 'HRAS', 'JUN', 'MAP2K1', 'MAP2K2', 'MAP2K4', 'MAP3K1', 'MAPK1', 'MAPK3', 'MAPK8', 'PAK1', 'PRKCA', 'PRKCB', 'PTK2B', 'RAC1', 'RAF1', 'SHC1', 'SOS1', 'ADAM12', 'AGT', 'AGTR2', 'EDN1', 'EDNRA', 'EDNRB', 'EGF', 'EGFR', 'FOS', 'HRAS', 'JUN', 'MYC', 'NFKB1', 'PLCG1', 'PRKCA', 'PRKCB', 'RELA', 'RHOA', 'GABRA1', 'GABRA2', 'GABRA3', 'GABRA4', 'GABRA5', 'GABRA6', 'GPX1', 'PRKCE', 'SOD1', 'ADRB2', 'AKT1', 'ANXA1', 'CALM1', 'CALM2', 'CALM3', 'CORIN', 'GNAS', 'HSP90AA1', 'NFKB1', 'NOS3', 'NPPA', 'NR3C1', 'PIK3CA', 'PIK3CG', 'PIK3R1', 'RELA', 'AKT1', 'AVP', 'CABIN1', 'CALM1', 'CALM2', 'CALM3', 'CAMK1', 'CAMK1G', 'HDAC5', 'IGF1', 'IGF1R', 'INSR', 'MAP2K6', 'MAPK14', 'MAPK7', 'MYOD1', 'NFATC1', 'NFATC2', 'PIK3CA', 'PIK3CG', 'PIK3R1', 'PPP3CA', 'PPP3CB', 'PPP3CC', 'YWHAH', 'ACTA1', 'AGT', 'AKT1', 'CALM1', 'CALM2', 'CALM3', 'CALR', 'CAMK1', 'CAMK1G', 'CAMK4', 'CREBBP', 'CSNK1A1', 'CTF1', 'EDN1', 'ELSPBP1', 'F2', 'FGF2', 'GATA4', 'GSK3B', 'HAND1', 'HAND2', 'HBEGF', 'HRAS', 'IGF1', 'MAP2K1', 'MAPK1', 'MAPK14', 'MAPK3', 'MAPK8', 'MEF2C', 'MYH2', 'NFATC1', 'NFATC2', 'NFATC3', 'NFATC4', 'NKX2-5', 'NPPA', 'PIK3CA', 'PIK3CG', 'PIK3R1', 'PPP3CA', 'PPP3CB', 'PPP3CC', 'PRKACB', 'PRKACG', 'PRKAR1A', 'PRKAR1B', 'PRKAR2A', 'PRKAR2B', 'RAF1', 'RPS6KB1', 'CALM1', 'CALM2', 'CALM3', 'CAMK1', 'CAMK1G', 'CAMK4', 'ESRRA', 'HDAC5', 'PPARA', 'PPARGC1A', 'PPP3CA', 'PPP3CB', 'PPP3CC', 'SLC2A4', 'YWHAH', 'APC', 'AXIN1', 'CREBBP', 'CTNNB1', 'DVL1', 'EP300', 'FZD1', 'GSK3B', 'HDAC1', 'KAT5', 'LDB1', 'LEF1', 'MED1', 'PITX2', 'TRRAP', 'WNT1', 'AHSP', 'COL4A1', 'COL4A2', 'COL4A3', 'COL4A4', 'COL4A5', 'COL4A6', 'F10', 'F2', 'F2R', 'F7', 'FGA', 'FGB', 'FGG', 'PLAT', 'PLG', 'PROC', 'PROS1', 'SERPINC1', 'TFPI', 'ABCB1', 'AKT1', 'ATM', 'BAX', 'CDKN1A', 'DNAJB1', 'EP300', 'FHL2', 'GADD45A', 'HIC1', 'HIF1A', 'HSP90AA1', 'HSPA1A', 'IGFBP3', 'MAPK8', 'MDM2', 'NFKBIB', 'NQO1', 'RPA1', 'TAF1', 'TP53', 'ACTA1', 'AKT1', 'BDKRB2', 'CALM1', 'CALM2', 'CALM3', 'CAV1', 'CHRM1', 'CHRNA1', 'FLT1', 'FLT4', 'HSP90AA1', 'KDR', 'KNG1', 'NOS3', 'PDE2A', 'PDE3A', 'PDE3B', 'PRKACB', 'PRKACG', 'PRKAR1A', 'PRKAR1B', 'PRKAR2A', 'PRKAR2B', 'RYR2', 'SLC7A1', 'TNNI1', 'VEGFA', 'ARNT', 'ASPH', 'COPS5', 'CREB1', 'EDN1', 'EP300', 'EPO', 'HIF1A', 'HSP90AA1', 'JUN', 'LDHA', 'NOS3', 'P4HB', 'VEGFA', 'VHL']\n"
     ]
    }
   ],
   "source": [
    "for set in geneSets:\n",
    "    print(geneSets[set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter gene sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredGeneSets = {}\n",
    "for setName in geneSets:\n",
    "    filteredGeneSets[setName] = genotypes[(genotypes['Gene'].isin(geneSets[setName]) & (genotypes['ExonicFunc'] == 'nonsynonymous_SNV'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alsod: (3021, 5923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/08 15:37:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardiac: (3319, 5923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/08 15:38:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/10/08 15:39:00 ERROR Executor: Exception in task 1.0 in stage 18.0 (TID 436)]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:476)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$$Lambda$2769/0x0000000800ea7950.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$153/0x00000008002c4428.apply(Unknown Source)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)\n",
      "\tat org.apache.spark.sql.execution.ProjectEvaluatorFactory$ProjectPartitionEvaluator.eval(ProjectEvaluatorFactory.scala:36)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:100)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec$$Lambda$4054/0x000000080110cd30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2987/0x0000000800f53360.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "23/10/08 15:39:00 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 18.0 (TID 436),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:476)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$$Lambda$2769/0x0000000800ea7950.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$153/0x00000008002c4428.apply(Unknown Source)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)\n",
      "\tat org.apache.spark.sql.execution.ProjectEvaluatorFactory$ProjectPartitionEvaluator.eval(ProjectEvaluatorFactory.scala:36)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:100)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec$$Lambda$4054/0x000000080110cd30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2987/0x0000000800f53360.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "23/10/08 15:39:00 WARN TaskSetManager: Lost task 1.0 in stage 18.0 (TID 436) (172.20.10.2 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:476)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$$Lambda$2769/0x0000000800ea7950.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$153/0x00000008002c4428.apply(Unknown Source)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)\n",
      "\tat org.apache.spark.sql.execution.ProjectEvaluatorFactory$ProjectPartitionEvaluator.eval(ProjectEvaluatorFactory.scala:36)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:100)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec$$Lambda$4054/0x000000080110cd30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2987/0x0000000800f53360.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\n",
      "23/10/08 15:39:00 ERROR TaskSetManager: Task 1 in stage 18.0 failed 1 times; aborting job\n",
      "23/10/08 15:39:00 ERROR FileFormatWriter: Aborting job c48e0aac-ec6a-4c04-8d65-a0cb3ca1f9b1.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 1 times, most recent failure: Lost task 1.0 in stage 18.0 (TID 436) (172.20.10.2 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:476)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$$Lambda$2769/0x0000000800ea7950.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$153/0x00000008002c4428.apply(Unknown Source)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)\n",
      "\tat org.apache.spark.sql.execution.ProjectEvaluatorFactory$ProjectPartitionEvaluator.eval(ProjectEvaluatorFactory.scala:36)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:100)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec$$Lambda$4054/0x000000080110cd30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2987/0x0000000800f53360.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:476)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$$Lambda$2769/0x0000000800ea7950.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$153/0x00000008002c4428.apply(Unknown Source)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)\n",
      "\tat org.apache.spark.sql.execution.ProjectEvaluatorFactory$ProjectPartitionEvaluator.eval(ProjectEvaluatorFactory.scala:36)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:100)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec$$Lambda$4054/0x000000080110cd30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2987/0x0000000800f53360.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/noot/Documents/case-control-genomics/adhoc analysis/geneSetExtraction.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noot/Documents/case-control-genomics/adhoc%20analysis/geneSetExtraction.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m setName \u001b[39min\u001b[39;00m filteredGeneSets:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noot/Documents/case-control-genomics/adhoc%20analysis/geneSetExtraction.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msetName\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfilteredGeneSets[setName]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/noot/Documents/case-control-genomics/adhoc%20analysis/geneSetExtraction.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     filteredGeneSets[setName]\u001b[39m.\u001b[39;49mreset_index()\u001b[39m.\u001b[39;49mto_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m{\u001b[39;49;00msetName\u001b[39m}\u001b[39;49;00m\u001b[39mGenotypes\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noot/Documents/case-control-genomics/adhoc%20analysis/geneSetExtraction.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m    \u001b[39m# Construct and run the concatenation command with awk to remove redundant headers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noot/Documents/case-control-genomics/adhoc%20analysis/geneSetExtraction.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     cat_command \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mawk \u001b[39m\u001b[39m'\u001b[39m\u001b[39m(NR == 1) || (FNR > 1)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m ./\u001b[39m\u001b[39m{\u001b[39;00msetName\u001b[39m}\u001b[39;00m\u001b[39mGenotypes/part*.csv > ./\u001b[39m\u001b[39m{\u001b[39;00msetName\u001b[39m}\u001b[39;00m\u001b[39mGenotypes.csv\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/pandas/generic.py:881\u001b[0m, in \u001b[0;36mFrame.to_csv\u001b[0;34m(self, path, sep, na_rep, columns, header, quotechar, date_format, escapechar, num_files, mode, partition_cols, index_col, **options)\u001b[0m\n\u001b[1;32m    872\u001b[0m     builder\u001b[39m.\u001b[39mpartitionBy(partition_cols)\n\u001b[1;32m    873\u001b[0m builder\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m    874\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m    875\u001b[0m     nullValue\u001b[39m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m     charToEscapeQuoteEscaping\u001b[39m=\u001b[39mescapechar,\n\u001b[1;32m    880\u001b[0m )\n\u001b[0;32m--> 881\u001b[0m builder\u001b[39m.\u001b[39;49moptions(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(path)\n\u001b[1;32m    882\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39;49mjava_exception)\n\u001b[1;32m    182\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:159\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mgetCause()\n\u001b[1;32m    157\u001b[0m stacktrace: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m jvm\u001b[39m.\u001b[39morg\u001b[39m.\u001b[39mapache\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mUtils\u001b[39m.\u001b[39mexceptionString(e)\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m c \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (\n\u001b[0;32m--> 159\u001b[0m     is_instance_of(gw, c, \u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.api.python.PythonException\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    160\u001b[0m     \u001b[39m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m    162\u001b[0m         \u001b[39mmap\u001b[39m(\n\u001b[1;32m    163\u001b[0m             \u001b[39mlambda\u001b[39;00m v: \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.execution.python\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39mtoString(), c\u001b[39m.\u001b[39mgetStackTrace()\n\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  An exception was thrown from the Python worker. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease see the stack trace below.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m c\u001b[39m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m PythonException(msg, stacktrace)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[39mreturn\u001b[39;00m gateway\u001b[39m.\u001b[39;49mjvm\u001b[39m.\u001b[39;49mpy4j\u001b[39m.\u001b[39mreflection\u001b[39m.\u001b[39mTypeUtil\u001b[39m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[39m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[39m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m does not exist in the JVM\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[39mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for setName in filteredGeneSets:\n",
    "    print(f\"{setName}: {filteredGeneSets[setName].shape}\")\n",
    "    filteredGeneSets[setName].reset_index().to_csv(f'./{setName}Genotypes', sep=\"\\t\", index=False)\n",
    "    \n",
    "   # Construct and run the concatenation command with awk to remove redundant headers\n",
    "    cat_command = f\"awk '(NR == 1) || (FNR > 1)' ./{setName}Genotypes/part*.csv > ./{setName}Genotypes.csv\"\n",
    "    subprocess.run(cat_command, shell=True, check=True)\n",
    "    \n",
    "    # Construct and run the removal command\n",
    "    rm_command = f\"rm -r ./{setName}Genotypes\"\n",
    "    subprocess.run(rm_command, shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noot/.pyenv/versions/miniconda3-latest/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "23/10/08 15:34:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/10/08 15:34:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='rm -r ./cardiacGenotypes', returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "filteredGeneSets['cardiac'].reset_index().to_csv(f'./cardiacGenotypes', sep=\"\\t\", index=False)\n",
    "# Construct and run the concatenation command with awk to remove redundant headers\n",
    "cat_command = f\"awk '(NR == 1) || (FNR > 1)' ./cardiacGenotypes/part*.csv > ./cardiacGenotypes.csv\"\n",
    "subprocess.run(cat_command, shell=True, check=True)\n",
    "\n",
    "# Construct and run the removal command\n",
    "rm_command = f\"rm -r ./cardiacGenotypes\"\n",
    "subprocess.run(rm_command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
